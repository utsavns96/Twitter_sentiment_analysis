{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obama Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we first specify all the imports that we'll need for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#models:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "training_data_excel = 'C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/training-Obama-Romney-tweets.xlsx'\n",
    "training_data_sheet = 'Obama'\n",
    "sample_data_excel = 'C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/sample-testdata.xlsx'\n",
    "sample_data_sheet = 'Obama'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data from the input excel into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7199, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1: positive, -1: negative, 0: neutral, 2: mixed</td>\n",
       "      <td>Class</td>\n",
       "      <td>Your class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:28:53-05:00</td>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-10 00:00:00</td>\n",
       "      <td>10:09:00-05:00</td>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:04:30-05:00</td>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:00:36-05:00</td>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date            time  \\\n",
       "0         NaN                  NaN             NaN   \n",
       "1         NaN  2012-10-16 00:00:00  10:28:53-05:00   \n",
       "2         NaN  2016-12-10 00:00:00  10:09:00-05:00   \n",
       "3         NaN  2012-10-16 00:00:00  10:04:30-05:00   \n",
       "4         NaN  2012-10-16 00:00:00  10:00:36-05:00   \n",
       "\n",
       "                                     Anootated tweet Unnamed: 4  Unnamed: 5  \n",
       "0    1: positive, -1: negative, 0: neutral, 2: mixed      Class  Your class  \n",
       "1  Kirkpatrick, who wore a baseball cap embroider...          0         NaN  \n",
       "2  Question: If <e>Romney</e> and <e>Obama</e> ha...          2         NaN  \n",
       "3  #<e>obama</e> debates that Cracker Ass Cracker...          1         NaN  \n",
       "4  RT @davewiner Slate: Blame <e>Obama</e> for fo...          2         NaN  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading\n",
    "data = pd.ExcelFile(training_data_excel)\n",
    "obama = pd.read_excel(data, training_data_sheet)\n",
    "print(obama.shape)\n",
    "obama.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start cleaning the data.<br>\n",
    "We drop the first row from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:28:53-05:00</td>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-10 00:00:00</td>\n",
       "      <td>10:09:00-05:00</td>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:04:30-05:00</td>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:00:36-05:00</td>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:50:08-05:00</td>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date            time  \\\n",
       "1         NaN  2012-10-16 00:00:00  10:28:53-05:00   \n",
       "2         NaN  2016-12-10 00:00:00  10:09:00-05:00   \n",
       "3         NaN  2012-10-16 00:00:00  10:04:30-05:00   \n",
       "4         NaN  2012-10-16 00:00:00  10:00:36-05:00   \n",
       "5         NaN  2012-10-16 00:00:00  09:50:08-05:00   \n",
       "\n",
       "                                     Anootated tweet Unnamed: 4 Unnamed: 5  \n",
       "1  Kirkpatrick, who wore a baseball cap embroider...          0        NaN  \n",
       "2  Question: If <e>Romney</e> and <e>Obama</e> ha...          2        NaN  \n",
       "3  #<e>obama</e> debates that Cracker Ass Cracker...          1        NaN  \n",
       "4  RT @davewiner Slate: Blame <e>Obama</e> for fo...          2        NaN  \n",
       "5  @Hollivan @hereistheanswer  Youre missing the ...          0        NaN  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama = obama[1:]\n",
    "obama.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can drop the columns that we do not need, namely: `Unnamed: 0`, `date`, `time` and `Unnamed: 5`.<br>\n",
    "We also rename `Unnamed: 4 ` to `class` and `Anootated tweet` to `tweet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet class\n",
       "1  Kirkpatrick, who wore a baseball cap embroider...     0\n",
       "2  Question: If <e>Romney</e> and <e>Obama</e> ha...     2\n",
       "3  #<e>obama</e> debates that Cracker Ass Cracker...     1\n",
       "4  RT @davewiner Slate: Blame <e>Obama</e> for fo...     2\n",
       "5  @Hollivan @hereistheanswer  Youre missing the ...     0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama = obama.drop(['Unnamed: 0', 'date', 'time', 'Unnamed: 5'], axis=1)\n",
    "obama = obama.rename(columns={'Unnamed: 4': 'class', 'Anootated tweet': 'tweet'})\n",
    "obama.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of classes available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1            1922\n",
      "0             1896\n",
      "1             1653\n",
      "2             1474\n",
      "0               82\n",
      "2               70\n",
      "-1              46\n",
      "1               26\n",
      "irrevelant      23\n",
      "irrelevant       1\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(obama['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we are only interested in the classes `-1, 0 and 1`. Therefore, we drop all the other classes from the dataframe.<br>\n",
    "We also change the column to be an integer, since the values are a mix of string and integers right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    1978\n",
      "-1    1968\n",
      " 1    1679\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "1  Kirkpatrick, who wore a baseball cap embroider...      0\n",
       "3  #<e>obama</e> debates that Cracker Ass Cracker...      1\n",
       "5  @Hollivan @hereistheanswer  Youre missing the ...      0\n",
       "7  I was raised as a Democrat  left the party yea...     -1\n",
       "8  The <e>Obama camp</e> can't afford to lower ex...      0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_df = obama[obama['class'].isin(['-1', '0', '1',-1,0,1])].copy(deep=True)\n",
    "obama_df['class']=obama_df['class'].astype(int)\n",
    "print(obama_df['class'].value_counts())\n",
    "obama_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = obama_df['class'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, alpha=0.8)\n",
    "\n",
    "plt.title('Number of Tweets by Class')\n",
    "plt.ylabel('Number of Tweets', fontsize=12)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = obama_df['tweet'].value_counts()\n",
    "# ax = obama_df.plot.bar(x='class', y=class_counts, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start working on the actual tweets. The first few steps that we need to perform are cleaning the text itself and tokenizing it.<br>\n",
    "For that, we use 2 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub(r'www.[^ ]+', '', text)\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = clean(text)\n",
    "    text = regexp.tokenize(text)\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below performs the task of going through our dataset, cleaning and tokenizing every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_df['tweet_token'] = obama_df['tweet'].apply(lambda stext: tokenize(str(stext)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove the words that are less than 2 characters, and appear less than 2 times, since those are most likely noise and would not contribute anything to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with length less than 2\n",
    "obama_df['tweet_string'] = obama_df['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "#Find a frequency distribution, and remove words with frequency less than 1\n",
    "all_words = ' '.join([text for text in obama_df['tweet_string']])\n",
    "tokenized_obama_df = nltk.tokenize.word_tokenize(all_words)\n",
    "fdist = FreqDist(tokenized_obama_df)\n",
    "obama_df['tweet_string_fdist'] = obama_df['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to perform the important task of Lemmatizing our dataset. To do this, we use `WordNetLemmatizer` with `Parts-Of-Speech tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\utsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatiser(text):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], pos_tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>tweet_string_fdist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wore cap barack obama signature look jason jou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kirkpatrick, wore, baseball, cap, embroidered...</td>\n",
       "      <td>kirkpatrick wore baseball cap embroidered bara...</td>\n",
       "      <td>wore cap barack obama signature look jason jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obama debate cracker as cracker tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>[e, obama, e, debates, cracker, ass, cracker, ...</td>\n",
       "      <td>obama debates cracker ass cracker tonight tuned</td>\n",
       "      <td>obama debates cracker ass cracker tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[youre, missing, point, im, afraid, understand...</td>\n",
       "      <td>youre missing point afraid understand bigger p...</td>\n",
       "      <td>missing point afraid understand bigger picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>raise democrat leave party year ago never see ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[raised, democrat, left, party, years, ago, li...</td>\n",
       "      <td>raised democrat left party years ago lifetime ...</td>\n",
       "      <td>raised democrat left party years ago never see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>obama camp afford low expectation tonight deba...</td>\n",
       "      <td>0</td>\n",
       "      <td>[e, obama, camp, e, afford, lower, expectation...</td>\n",
       "      <td>obama camp afford lower expectations tonight d...</td>\n",
       "      <td>obama camp afford lower expectations tonight d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class  \\\n",
       "1  wore cap barack obama signature look jason jou...      0   \n",
       "3            obama debate cracker as cracker tonight      1   \n",
       "5  miss point afraid understand big picture dont ...      0   \n",
       "7  raise democrat leave party year ago never see ...     -1   \n",
       "8  obama camp afford low expectation tonight deba...      0   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "1  [kirkpatrick, wore, baseball, cap, embroidered...   \n",
       "3  [e, obama, e, debates, cracker, ass, cracker, ...   \n",
       "5  [youre, missing, point, im, afraid, understand...   \n",
       "7  [raised, democrat, left, party, years, ago, li...   \n",
       "8  [e, obama, camp, e, afford, lower, expectation...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "1  kirkpatrick wore baseball cap embroidered bara...   \n",
       "3    obama debates cracker ass cracker tonight tuned   \n",
       "5  youre missing point afraid understand bigger p...   \n",
       "7  raised democrat left party years ago lifetime ...   \n",
       "8  obama camp afford lower expectations tonight d...   \n",
       "\n",
       "                                  tweet_string_fdist  \n",
       "1  wore cap barack obama signature look jason jou...  \n",
       "3          obama debates cracker ass cracker tonight  \n",
       "5  missing point afraid understand bigger picture...  \n",
       "7  raised democrat left party years ago never see...  \n",
       "8  obama camp afford lower expectations tonight d...  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_df['tweet'] = obama_df['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "obama_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the columns `tweet_token`, `tweet_string` and `tweet_string_fdist` now.<br>\n",
    "We also remove the null values, to make sure that we do not have any empty records left after the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wore cap barack obama signature look jason jou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obama debate cracker as cracker tonight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>raise democrat leave party year ago never see ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>obama camp afford low expectation tonight deba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "1  wore cap barack obama signature look jason jou...      0\n",
       "3            obama debate cracker as cracker tonight      1\n",
       "5  miss point afraid understand big picture dont ...      0\n",
       "7  raise democrat leave party year ago never see ...     -1\n",
       "8  obama camp afford low expectation tonight deba...      0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_df = obama_df.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "obama_df.dropna(inplace=True)\n",
    "print(obama_df.shape)\n",
    "obama_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at the distribution of the classes in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    1978\n",
      "-1    1968\n",
      " 1    1679\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(obama_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for processing. To train and test our models, we will perform a train-test-split of 80-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = obama_df['tweet']\n",
    "df_Y = obama_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X,df_Y,test_size=0.2,random_state = 1551)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `TfidfVectorizer` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,2))\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try using `Word2Vec` from `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "obama_df['clean_text_tok']=[nltk.word_tokenize(i) for i in obama_df['tweet']] \n",
    "model = Word2Vec(obama_df['clean_text_tok'],min_count=1) \n",
    "w2v = dict(zip(model.wv.index_to_key , model.wv.vectors))   \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)\n",
    "obama_df.drop(['clean_text_tok'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the negative numbers produced by this do not trip up our models, we use `MinMaxScaler` to normalize the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_vectors_w2v = scaler.fit_transform(X_train_vectors_w2v)\n",
    "X_test_vectors_w2v = scaler.transform(X_test_vectors_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create and test on our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start running our models, to make things easier, we will create a dataframe to keep a track of our performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(columns=['Model','Vectorization','Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things even easier, I have written a small function to store our metrics in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metrics(model_name,vector_name,test,predictions):\n",
    "    global performance\n",
    "    new_data = {'Model': model_name,\n",
    "                'Vectorization': vector_name,\n",
    "                'Accuracy': round(accuracy_score(test,predictions),4),\n",
    "                'Precision': round(precision_score(test,predictions, average='weighted'),4),\n",
    "                'Recall': round(recall_score(test,predictions, average='weighted'),4),\n",
    "                'F1 Score': round(f1_score(test,predictions, average='weighted'),4)}\n",
    "    performance = performance.append(new_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.61      0.61       428\n",
      "           0       0.49      0.52      0.51       365\n",
      "           1       0.62      0.57      0.59       332\n",
      "\n",
      "    accuracy                           0.57      1125\n",
      "   macro avg       0.57      0.57      0.57      1125\n",
      "weighted avg       0.57      0.57      0.57      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "lr_model_tfidf = LogisticRegression(solver='saga',C=5,penalty='l2',random_state=44) #4=57%\n",
    "lr_model_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "lr_tfidf_y_pred = lr_model_tfidf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,lr_tfidf_y_pred))\n",
    "write_metrics('Logistic Regression','TF-IDF',y_test,lr_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.52      0.54      0.53       428\n",
      "           0       0.43      0.49      0.46       365\n",
      "           1       0.58      0.46      0.51       332\n",
      "\n",
      "    accuracy                           0.50      1125\n",
      "   macro avg       0.51      0.50      0.50      1125\n",
      "weighted avg       0.51      0.50      0.50      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "lr_model_w2v = LogisticRegression(solver='liblinear',C=10,penalty='l2',random_state=4) #4=57%\n",
    "lr_model_w2v.fit(X_train_vectors_w2v, y_train)\n",
    "lr_w2v_y_pred = lr_model_w2v.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,lr_w2v_y_pred))\n",
    "write_metrics('Logistic Regression','Word2Vec',y_test,lr_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.65      0.63       428\n",
      "           0       0.49      0.54      0.52       365\n",
      "           1       0.67      0.53      0.59       332\n",
      "\n",
      "    accuracy                           0.58      1125\n",
      "   macro avg       0.59      0.58      0.58      1125\n",
      "weighted avg       0.59      0.58      0.58      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf_model = MultinomialNB()\n",
    "nb_tfidf_model.fit(X_train_vectors_tfidf, y_train)\n",
    "nb_tfidf_y_pred = nb_tfidf_model.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,nb_tfidf_y_pred))\n",
    "write_metrics('Naive Bayes','TF-IDF',y_test,nb_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.45      0.47      0.46       428\n",
      "           0       0.32      0.25      0.28       365\n",
      "           1       0.40      0.47      0.43       332\n",
      "\n",
      "    accuracy                           0.40      1125\n",
      "   macro avg       0.39      0.40      0.39      1125\n",
      "weighted avg       0.39      0.40      0.39      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "nb_w2v_model = MultinomialNB()\n",
    "nb_w2v_model.fit(X_train_vectors_w2v, y_train)\n",
    "nb_w2v_y_pred = nb_w2v_model.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,nb_w2v_y_pred))\n",
    "write_metrics('Naive Bayes','Word2Vec',y_test,nb_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.62      0.61       428\n",
      "           0       0.50      0.53      0.51       365\n",
      "           1       0.63      0.58      0.61       332\n",
      "\n",
      "    accuracy                           0.58      1125\n",
      "   macro avg       0.58      0.58      0.58      1125\n",
      "weighted avg       0.58      0.58      0.58      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_model = svm.SVC(kernel='linear', random_state=1564)\n",
    "svm_tfidf_model.fit(X_train_vectors_tfidf, y_train)\n",
    "svm_tfidf_y_pred = svm_tfidf_model.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,svm_tfidf_y_pred))\n",
    "write_metrics('SVM','TF-IDF',y_test,svm_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.52      0.52       428\n",
      "           0       0.39      0.52      0.45       365\n",
      "           1       0.60      0.36      0.45       332\n",
      "\n",
      "    accuracy                           0.47      1125\n",
      "   macro avg       0.50      0.47      0.47      1125\n",
      "weighted avg       0.50      0.47      0.47      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "svm_w2v_model = svm.SVC(kernel='linear', random_state=4)\n",
    "svm_w2v_model.fit(X_train_vectors_w2v, y_train)\n",
    "svm_w2v_y_pred = svm_w2v_model.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,svm_w2v_y_pred))\n",
    "write_metrics('SVM','Word2Vec',y_test,svm_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.32      0.43       428\n",
      "           0       0.50      0.45      0.48       365\n",
      "           1       0.43      0.77      0.55       332\n",
      "\n",
      "    accuracy                           0.50      1125\n",
      "   macro avg       0.53      0.51      0.49      1125\n",
      "weighted avg       0.54      0.50      0.48      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "knn_tfidf = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_tfidf_gscv = GridSearchCV(knn_tfidf, param_grid, cv=5)#fit model to data\n",
    "clf = knn_tfidf_gscv.fit(X_train_vectors_tfidf, y_train)\n",
    "knn_tfidf_y_pred = clf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,knn_tfidf_y_pred))\n",
    "write_metrics('KNN','TF-IDF',y_test,knn_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.59      0.52       428\n",
      "           0       0.36      0.37      0.37       365\n",
      "           1       0.46      0.30      0.36       332\n",
      "\n",
      "    accuracy                           0.43      1125\n",
      "   macro avg       0.43      0.42      0.42      1125\n",
      "weighted avg       0.43      0.43      0.42      1125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_22920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "knn_w2v = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_w2v_gscv = GridSearchCV(knn_w2v, param_grid, cv=5)#fit model to data\n",
    "clf_w2v = knn_w2v_gscv.fit(X_train_vectors_w2v, y_train)\n",
    "knn_w2v_y_pred = clf_w2v.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,knn_w2v_y_pred))\n",
    "write_metrics('KNN','Word2Vec',y_test,knn_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.5013</td>\n",
       "      <td>0.5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.5880</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.5812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.3991</td>\n",
       "      <td>0.3921</td>\n",
       "      <td>0.3991</td>\n",
       "      <td>0.3929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5787</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.5787</td>\n",
       "      <td>0.5794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>0.4742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.4951</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>0.4951</td>\n",
       "      <td>0.4821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.4224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Vectorization  Accuracy  Precision  Recall  F1 Score\n",
       "0  Logistic Regression        TF-IDF    0.5698     0.5726  0.5698    0.5706\n",
       "1  Logistic Regression      Word2Vec    0.5013     0.5096  0.5013    0.5026\n",
       "2          Naive Bayes        TF-IDF    0.5804     0.5880  0.5804    0.5812\n",
       "3          Naive Bayes      Word2Vec    0.3991     0.3921  0.3991    0.3929\n",
       "4                  SVM        TF-IDF    0.5787     0.5810  0.5787    0.5794\n",
       "5                  SVM      Word2Vec    0.4738     0.4980  0.4738    0.4742\n",
       "6                  KNN        TF-IDF    0.4951     0.5432  0.4951    0.4821\n",
       "7                  KNN      Word2Vec    0.4302     0.4302  0.4302    0.4224"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this performance data, we can see that Logistic Regression, Naive Bayes and SVM have performance that is very close. Therefore, I use a Voting Classifier to create a model that predicts based on the votes from all 3 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.63      0.62       428\n",
      "           0       0.49      0.52      0.51       365\n",
      "           1       0.62      0.57      0.59       332\n",
      "\n",
      "    accuracy                           0.57      1125\n",
      "   macro avg       0.58      0.57      0.57      1125\n",
      "weighted avg       0.58      0.57      0.57      1125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = [] \n",
    "estimator.append(('LR', LogisticRegression(solver='saga',C=5,penalty='l2',random_state=44))) \n",
    "estimator.append(('Naive Bayes', MultinomialNB())) \n",
    "estimator.append(('SVM', svm.SVC(kernel='linear', random_state=4, probability=True)))\n",
    "voting = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "voting.fit(X_train_vectors_tfidf, y_train) \n",
    "y_pred_vot = voting.predict(X_test_vectors_tfidf) \n",
    "print(classification_report(y_test,y_pred_vot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the statistics, we achieve a 57% accuracy using our Voting classifier. But, can we improve things further?<br>\n",
    "If we look back to the start of this notebook, we dropped all records that were not in the classes -1, 0 and 1.<br>\n",
    "However, that means we lose out on 1,544 records that are labelled as class 2 (mixed). These tweets express both positive and negative opinions.<br>\n",
    "In our problem statement, we are only concered with the classes -1, 0 and 1. \n",
    "So, what if we were to perform semi-supervised learning and treat these as unlabelled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1544, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question romney obama child mitt punch five ob...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slate blame obama four death libya blame bush ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mitt romney make money barack obama make money...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tonight debate game feel pres obama call romne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>guy rather obama critique romney tax plan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "2   question romney obama child mitt punch five ob...      2\n",
       "4   slate blame obama four death libya blame bush ...      2\n",
       "6   mitt romney make money barack obama make money...      2\n",
       "9   tonight debate game feel pres obama call romne...      2\n",
       "12          guy rather obama critique romney tax plan      2"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_mixed = obama[obama['class'].isin(['2',2])].copy(deep=True)\n",
    "obama_mixed['class']=obama_mixed['class'].astype(int)\n",
    "obama_mixed['tweet_token'] = obama_mixed['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "\n",
    "#remove words with length less than 2\n",
    "obama_mixed['tweet_string'] = obama_mixed['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "\n",
    "all_words = ' '.join([text for text in obama_mixed['tweet_string']])\n",
    "tokenized_obama_mixed = nltk.tokenize.word_tokenize(all_words)\n",
    "fdist = FreqDist(tokenized_obama_mixed)\n",
    "obama_mixed['tweet_string_fdist'] = obama_mixed['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "\n",
    "obama_mixed['tweet'] = obama_mixed['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "obama_mixed.head(5)\n",
    "obama_mixed = obama_mixed.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "obama_mixed.dropna(inplace=True)\n",
    "obama_mixed.drop(['class'], axis=1)\n",
    "print(obama_mixed.shape)\n",
    "obama_mixed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-IDF vectors of these records,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_vectors_tfidf = tfidf_vectorizer.transform(obama_mixed['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the classes for these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1  0 ...  1  0  1]\n"
     ]
    }
   ],
   "source": [
    "mixed_y_pred = voting.predict(mixed_vectors_tfidf)\n",
    "print(mixed_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add these classes to our `obama_mixed` dataframe,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question romney obama child mitt punch five ob...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slate blame obama four death libya blame bush ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mitt romney make money barack obama make money...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tonight debate game feel pres obama call romne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>guy rather obama critique romney tax plan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "2   question romney obama child mitt punch five ob...      0\n",
       "4   slate blame obama four death libya blame bush ...     -1\n",
       "6   mitt romney make money barack obama make money...      0\n",
       "9   tonight debate game feel pres obama call romne...      0\n",
       "12          guy rather obama critique romney tax plan      0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_mixed['class'] = mixed_y_pred\n",
    "obama_mixed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And merge these records with our existing dataframe `obama_df`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7168, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_merged = obama_df.merge(obama_mixed, how='outer')\n",
    "obama_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we retrain our Voting Classifier on this new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df_X = obama_merged['tweet']\n",
    "ss_df_Y = obama_merged['class']\n",
    "ss_X_train, ss_X_test, ss_y_train, ss_y_test = train_test_split(ss_df_X,ss_df_Y,test_size=0.2,random_state = 1551)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(ss_X_train)\n",
    "ss_X_test_vectors_tfidf = tfidf_vectorizer.transform(ss_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.62      0.64       463\n",
      "           0       0.65      0.74      0.69       580\n",
      "           1       0.71      0.63      0.67       391\n",
      "\n",
      "    accuracy                           0.67      1434\n",
      "   macro avg       0.68      0.66      0.67      1434\n",
      "weighted avg       0.67      0.67      0.67      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator_ss = [] \n",
    "estimator_ss.append(('LR', LogisticRegression(solver='saga',C=5,penalty='l2',random_state=44))) \n",
    "estimator_ss.append(('Naive Bayes', MultinomialNB())) \n",
    "estimator_ss.append(('SVM', svm.SVC(kernel='linear', random_state=4, probability=True))) \n",
    "voting.fit(ss_X_train_vectors_tfidf, ss_y_train) \n",
    "y_pred_vot_ss = voting.predict(ss_X_test_vectors_tfidf) \n",
    "print(classification_report(ss_y_test,y_pred_vot_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This achieves 67% accuracy for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is ready, we can load, clean and tokenize the sample data excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wore cap barack obama signature look jason jou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obama debate cracker as cracker tonight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>raise democrat leave party year ago never see ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>obama camp afford low expectation tonight deba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "1  wore cap barack obama signature look jason jou...      0\n",
       "3            obama debate cracker as cracker tonight      1\n",
       "5  miss point afraid understand big picture dont ...      0\n",
       "7  raise democrat leave party year ago never see ...     -1\n",
       "8  obama camp afford low expectation tonight deba...      0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading\n",
    "sample_data = pd.ExcelFile(sample_data_excel)\n",
    "sample_obama = pd.read_excel(sample_data, sample_data_sheet)\n",
    "#print(sample_obama.shape)\n",
    "#print(sample_obama.head(5))\n",
    "if(len(sample_obama.columns) == 6):\n",
    "    sample_obama = sample_obama[1:]\n",
    "    sample_obama = sample_obama.drop(['Unnamed: 0', 'date', 'time', 'Unnamed: 5'], axis=1)\n",
    "    sample_obama = sample_obama.rename(columns={'Unnamed: 4': 'class', 'Anootated tweet': 'tweet'})\n",
    "    sample_obama = sample_obama[sample_obama['class'].isin(['-1', '0', '1',-1,0,1])]\n",
    "    sample_obama['class']=sample_obama['class'].astype(int)\n",
    "    #create a copy to use for printing to output file\n",
    "    output_data = sample_obama.copy(deep=True)\n",
    "    sample_obama['tweet_token'] = sample_obama['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "    #remove words with length less than 2\n",
    "    sample_obama['tweet_string'] = sample_obama['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "    #Find a frequency distribution, and remove words with frequency less than 1\n",
    "    all_words = ' '.join([text for text in sample_obama['tweet_string']])\n",
    "    tokenized_sample_obama = nltk.tokenize.word_tokenize(all_words)\n",
    "    fdist = FreqDist(tokenized_sample_obama)\n",
    "    sample_obama['tweet_string_fdist'] = sample_obama['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "    sample_obama['tweet'] = sample_obama['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "    #sample_obama.head(5)\n",
    "    sample_obama = sample_obama.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "    sample_obama.dropna(inplace=True)\n",
    "    #print(sample_obama.shape)\n",
    "    #sample_obama.head(5)\n",
    "    sample_df_X = sample_obama['tweet']\n",
    "    sample_df_Y = sample_obama['class']\n",
    "elif(len(sample_obama.columns) == 2):\n",
    "    sample_obama = pd.read_excel(sample_data, sample_data_sheet,header=None)\n",
    "    sample_obama.drop(sample_obama.columns[0], axis=1, inplace=True)\n",
    "    sample_obama.columns = ['tweet']\n",
    "    output_data = sample_obama.copy(deep=True)\n",
    "    sample_obama['tweet_token'] = sample_obama['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "    #remove words with length less than 2\n",
    "    sample_obama['tweet_string'] = sample_obama['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "    #Find a frequency distribution, and remove words with frequency less than 1\n",
    "    all_words = ' '.join([text for text in sample_obama['tweet_string']])\n",
    "    tokenized_sample_obama = nltk.tokenize.word_tokenize(all_words)\n",
    "    fdist = FreqDist(tokenized_sample_obama)\n",
    "    sample_obama['tweet_string_fdist'] = sample_obama['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "    sample_obama['tweet'] = sample_obama['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "    #sample_obama.head(5)\n",
    "    sample_obama = sample_obama.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "    sample_obama.dropna(inplace=True)\n",
    "    #print(sample_obama.shape)\n",
    "    #sample_obama.head(5)\n",
    "    sample_df_X = sample_obama['tweet']\n",
    "else:\n",
    "    print(\"Invalid columns - check the excel file\")\n",
    "X_sample_vectors = tfidf_vectorizer.transform(sample_df_X)\n",
    "print(sample_obama.shape)\n",
    "sample_obama.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data loading\n",
    "# sample_data = pd.ExcelFile(sample_data_excel)\n",
    "# sample_obama = pd.read_excel(sample_data, sample_data_sheet)\n",
    "# sample_obama = sample_obama[1:]\n",
    "# sample_obama = sample_obama.drop(['Unnamed: 0', 'date', 'time', 'Unnamed: 5'], axis=1)\n",
    "# sample_obama = sample_obama.rename(columns={'Unnamed: 4': 'class', 'Anootated tweet': 'tweet'})\n",
    "# sample_obama.head(5)\n",
    "# sample_obama = sample_obama[sample_obama['class'].isin(['-1', '0', '1',-1,0,1])]\n",
    "# sample_obama['class']=sample_obama['class'].astype(int)\n",
    "# #create a copy to use for printing to output file\n",
    "# output_data = sample_obama.copy(deep=True)\n",
    "# sample_obama['tweet_token'] = sample_obama['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "# #remove words with length less than 2\n",
    "# sample_obama['tweet_string'] = sample_obama['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "# #Find a frequency distribution, and remove words with frequency less than 1\n",
    "# all_words = ' '.join([text for text in sample_obama['tweet_string']])\n",
    "# tokenized_sample_obama = nltk.tokenize.word_tokenize(all_words)\n",
    "# fdist = FreqDist(tokenized_sample_obama)\n",
    "# sample_obama['tweet_string_fdist'] = sample_obama['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "# sample_obama['tweet'] = sample_obama['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "# sample_obama.head(5)\n",
    "# sample_obama = sample_obama.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "# sample_obama.dropna(inplace=True)\n",
    "# print(sample_obama.shape)\n",
    "# sample_obama.head(5)\n",
    "# sample_df_X = sample_obama['tweet']\n",
    "# sample_df_Y = sample_obama['class']\n",
    "# X_sample_vectors = tfidf_vectorizer.transform(sample_df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for our sample test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_y_pred = voting.predict(X_sample_vectors)\n",
    "# print(classification_report(sample_df_Y,sample_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our predicted classes to an excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data['Predicted class'] = sample_y_pred\n",
    "output_data.to_excel('C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/output_obama.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:<br>\n",
    "https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/ <br>\n",
    "https://medium.com/@qacbustamante/natural-language-processing-using-python-part-1-958d4ea1846e <br>\n",
    "https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/ <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
