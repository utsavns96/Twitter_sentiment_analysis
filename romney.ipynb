{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romney Tweets<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we first specify all the imports that we'll need for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands in the terminal to install the required packages\n",
    "\n",
    "`pip install gensim` <br>\n",
    "`pip install nltk` <br>\n",
    "`pip install seaborn` <br>\n",
    "`pip install sklearn` <br>\n",
    "`pip install pandas` <br>\n",
    "`pip install numpy` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#models:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Setting our file paths\n",
    "training_data_excel = 'C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/training-Obama-Romney-tweets.xlsx'\n",
    "training_data_sheet = 'Romney'\n",
    "sample_data_excel = 'C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/Test/final-testData-no-label-Romney-tweets.xlsx'\n",
    "sample_data_sheet = 'Romney'\n",
    "output_file = 'C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/utsav_Romney.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data from the input excel into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1: positive, -1: negative, 0: neutral, 2: mixed</td>\n",
       "      <td>Class</td>\n",
       "      <td>Your class label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:38:08-05:00</td>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:22:34-05:00</td>\n",
       "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:14:18-05:00</td>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:27:16-05:00</td>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date            time  \\\n",
       "0         NaN                  NaN             NaN   \n",
       "1         NaN  2012-10-16 00:00:00  09:38:08-05:00   \n",
       "2         NaN  2012-10-16 00:00:00  10:22:34-05:00   \n",
       "3         NaN  2012-10-16 00:00:00  10:14:18-05:00   \n",
       "4         NaN  2012-10-16 00:00:00  09:27:16-05:00   \n",
       "\n",
       "                                     Anootated tweet Unnamed: 4  \\\n",
       "0    1: positive, -1: negative, 0: neutral, 2: mixed      Class   \n",
       "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...         -1   \n",
       "2  Senior <e>Romney</e> Advisor Claims <e>Obama</...          2   \n",
       "3  .@WardBrenda @shortwave8669 @allanbourdius you...         -1   \n",
       "4  <e>Mitt Romney</e> still doesn't <a>believe</a...         -1   \n",
       "\n",
       "         Unnamed: 5  \n",
       "0  Your class label  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading\n",
    "data = pd.ExcelFile(training_data_excel)\n",
    "romney = pd.read_excel(data, training_data_sheet)\n",
    "romney.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Lemmatization<br>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start cleaning the data.<br>\n",
    "We drop the first row from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:38:08-05:00</td>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:22:34-05:00</td>\n",
       "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:14:18-05:00</td>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>09:27:16-05:00</td>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-10-16 00:00:00</td>\n",
       "      <td>10:11:43-05:00</td>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date            time  \\\n",
       "1         NaN  2012-10-16 00:00:00  09:38:08-05:00   \n",
       "2         NaN  2012-10-16 00:00:00  10:22:34-05:00   \n",
       "3         NaN  2012-10-16 00:00:00  10:14:18-05:00   \n",
       "4         NaN  2012-10-16 00:00:00  09:27:16-05:00   \n",
       "5         NaN  2012-10-16 00:00:00  10:11:43-05:00   \n",
       "\n",
       "                                     Anootated tweet Unnamed: 4 Unnamed: 5  \n",
       "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...         -1        NaN  \n",
       "2  Senior <e>Romney</e> Advisor Claims <e>Obama</...          2        NaN  \n",
       "3  .@WardBrenda @shortwave8669 @allanbourdius you...         -1        NaN  \n",
       "4  <e>Mitt Romney</e> still doesn't <a>believe</a...         -1        NaN  \n",
       "5  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...         -1        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney = romney[1:]\n",
    "romney.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can drop the columns that we do not need, namely: `Unnamed: 0`, `date`, `time` and `Unnamed: 5`.<br>\n",
    "We also rename `Unnamed: 4 ` to `class` and `Anootated tweet` to `tweet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet class\n",
       "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...    -1\n",
       "2  Senior <e>Romney</e> Advisor Claims <e>Obama</...     2\n",
       "3  .@WardBrenda @shortwave8669 @allanbourdius you...    -1\n",
       "4  <e>Mitt Romney</e> still doesn't <a>believe</a...    -1\n",
       "5  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...    -1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney = romney.drop(['Unnamed: 0', 'date', 'time', 'Unnamed: 5'], axis=1)\n",
    "romney = romney.rename(columns={'Unnamed: 4': 'class', 'Anootated tweet': 'tweet'})\n",
    "romney.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of classes available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      2893\n",
      "0       1680\n",
      "2       1351\n",
      "1       1075\n",
      "!!!!     169\n",
      "IR         3\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(romney['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we are only interested in the classes `-1, 0 and 1`. Therefore, we drop all the other classes from the dataframe.<br>\n",
    "We also change the column to be an integer, since the values are a mix of string and integers right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    2893\n",
      " 0    1680\n",
      " 1    1075\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...     -1\n",
       "3  .@WardBrenda @shortwave8669 @allanbourdius you...     -1\n",
       "4  <e>Mitt Romney</e> still doesn't <a>believe</a...     -1\n",
       "5  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...     -1\n",
       "6  Hope <e>Romney</e> debate prepped w/ the same ...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_df = romney[romney['class'].isin(['-1', '0', '1',-1,0,1])].copy(deep=True)\n",
    "romney_df['class']=romney_df['class'].astype(int)\n",
    "print(romney_df['class'].value_counts())\n",
    "romney_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start working on the actual tweets. The first few steps that we need to perform are cleaning the text itself and tokenizing it.<br>\n",
    "For that, we use 2 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n",
    "    text = re.sub(r'www.[^ ]+', '', text)\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = clean(text)\n",
    "    text = regexp.tokenize(text)\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below performs the task of going through our dataset, cleaning and tokenizing every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "romney_df['tweet_token'] = romney_df['tweet'].apply(lambda stext: tokenize(str(stext)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then remove the words that are less than 2 characters, and appear less than 2 times, since those are most likely noise and would not contribute anything to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with length less than 2\n",
    "romney_df['tweet_string'] = romney_df['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "#Find a frequency distribution, and remove words with frequency less than 1\n",
    "all_words = ' '.join([text for text in romney_df['tweet_string']])\n",
    "tokenized_romney_df = nltk.tokenize.word_tokenize(all_words)\n",
    "fdist = FreqDist(tokenized_romney_df)\n",
    "romney_df['tweet_string_fdist'] = romney_df['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to perform the important task of Lemmatizing our dataset. To do this, we use `WordNetLemmatizer` with `Parts-Of-Speech tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\utsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatiser(text):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], pos_tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_string</th>\n",
       "      <th>tweet_string_fdist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mitt romney bain help philip morris get high s...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[insidious, e, mitt, romney, e, bain, helped, ...</td>\n",
       "      <td>insidious mitt romney bain helped philip morri...</td>\n",
       "      <td>mitt romney bain helped philip morris get high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>-1</td>\n",
       "      <td>[mean, like, e, romney, e, cheated, primary]</td>\n",
       "      <td>mean like romney cheated primary</td>\n",
       "      <td>mean like romney cheated primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>-1</td>\n",
       "      <td>[e, mitt, romney, e, still, believe, black, pr...</td>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>romney tax plan deserve look secret one differ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[e, romney, e, tax, plan, deserves, nd, look, ...</td>\n",
       "      <td>romney tax plan deserves look secret one diffe...</td>\n",
       "      <td>romney tax plan deserves look secret one diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hope romney debate people last time</td>\n",
       "      <td>1</td>\n",
       "      <td>[hope, e, romney, e, debate, prepped, w, peopl...</td>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>hope romney debate people last time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class  \\\n",
       "1  mitt romney bain help philip morris get high s...     -1   \n",
       "3                     mean like romney cheat primary     -1   \n",
       "4          mitt romney still believe black president     -1   \n",
       "5  romney tax plan deserve look secret one differ...     -1   \n",
       "6                hope romney debate people last time      1   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "1  [insidious, e, mitt, romney, e, bain, helped, ...   \n",
       "3       [mean, like, e, romney, e, cheated, primary]   \n",
       "4  [e, mitt, romney, e, still, believe, black, pr...   \n",
       "5  [e, romney, e, tax, plan, deserves, nd, look, ...   \n",
       "6  [hope, e, romney, e, debate, prepped, w, peopl...   \n",
       "\n",
       "                                        tweet_string  \\\n",
       "1  insidious mitt romney bain helped philip morri...   \n",
       "3                   mean like romney cheated primary   \n",
       "4          mitt romney still believe black president   \n",
       "5  romney tax plan deserves look secret one diffe...   \n",
       "6        hope romney debate prepped people last time   \n",
       "\n",
       "                                  tweet_string_fdist  \n",
       "1  mitt romney bain helped philip morris get high...  \n",
       "3                   mean like romney cheated primary  \n",
       "4          mitt romney still believe black president  \n",
       "5  romney tax plan deserves look secret one diffe...  \n",
       "6                hope romney debate people last time  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_df['tweet'] = romney_df['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "romney_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the columns `tweet_token`, `tweet_string` and `tweet_string_fdist` now.<br>\n",
    "We also remove the null values, to make sure that we do not have any empty records left after the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5648, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mitt romney bain help philip morris get high s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>romney tax plan deserve look secret one differ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hope romney debate people last time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "1  mitt romney bain help philip morris get high s...     -1\n",
       "3                     mean like romney cheat primary     -1\n",
       "4          mitt romney still believe black president     -1\n",
       "5  romney tax plan deserve look secret one differ...     -1\n",
       "6                hope romney debate people last time      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_df = romney_df.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "romney_df.dropna(inplace=True)\n",
    "print(romney_df.shape)\n",
    "romney_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at the distribution of the classes in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    2893\n",
      " 0    1680\n",
      " 1    1075\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(romney_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = romney_df['class'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, alpha=0.8)\n",
    "\n",
    "plt.title('Number of Tweets by Class')\n",
    "plt.ylabel('Number of Tweets', fontsize=12)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for processing. To train and test our models, we will perform a train-test-split of 80-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = romney_df['tweet']\n",
    "df_Y = romney_df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X,df_Y,test_size=0.2,random_state = 1551)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization<br>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `TfidfVectorizer` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,2))\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try using `Word2Vec` from `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "romney_df['tweet_w2v_token']=[nltk.word_tokenize(i) for i in romney_df['tweet']] \n",
    "model = Word2Vec(romney_df['tweet_w2v_token'],min_count=1) \n",
    "w2v = dict(zip(model.wv.index_to_key , model.wv.vectors))   \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.fit(X_train_tok,y_train)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)\n",
    "romney_df.drop(['tweet_w2v_token'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the negative numbers produced by this do not trip up our models, we use `MinMaxScaler` to normalize the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_vectors_w2v = scaler.fit_transform(X_train_vectors_w2v)\n",
    "X_test_vectors_w2v = scaler.transform(X_test_vectors_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create and test on our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning:<br>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start running our models, to make things easier, we will create a dataframe to keep a track of our performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(columns=['Model','Vectorization','Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things even easier, I have written a small function to store our metrics in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metrics(model_name,vector_name,test,predictions):\n",
    "    global performance\n",
    "    new_data = {'Model': model_name,\n",
    "                'Vectorization': vector_name,\n",
    "                'Accuracy': round(accuracy_score(test,predictions),4),\n",
    "                'Precision': round(precision_score(test,predictions, average='weighted'),4),\n",
    "                'Recall': round(recall_score(test,predictions, average='weighted'),4),\n",
    "                'F1 Score': round(f1_score(test,predictions, average='weighted'),4)}\n",
    "    performance = performance.append(new_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.78      0.70       579\n",
      "           0       0.46      0.38      0.42       333\n",
      "           1       0.53      0.34      0.41       218\n",
      "\n",
      "    accuracy                           0.58      1130\n",
      "   macro avg       0.54      0.50      0.51      1130\n",
      "weighted avg       0.56      0.58      0.56      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "lr_model_tfidf = LogisticRegression(solver='saga',C=5,penalty='l2',random_state=44) #4=57%\n",
    "lr_model_tfidf.fit(X_train_vectors_tfidf, y_train)\n",
    "lr_tfidf_y_pred = lr_model_tfidf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,lr_tfidf_y_pred))\n",
    "write_metrics('Logistic Regression','TF-IDF',y_test,lr_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.92      0.68       579\n",
      "           0       0.44      0.16      0.23       333\n",
      "           1       0.55      0.05      0.09       218\n",
      "\n",
      "    accuracy                           0.53      1130\n",
      "   macro avg       0.51      0.38      0.33      1130\n",
      "weighted avg       0.51      0.53      0.43      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "lr_model_w2v = LogisticRegression(solver='liblinear',C=10,penalty='l2',random_state=4) #4=57%\n",
    "lr_model_w2v.fit(X_train_vectors_w2v, y_train)\n",
    "lr_w2v_y_pred = lr_model_w2v.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,lr_w2v_y_pred))\n",
    "write_metrics('Logistic Regression','Word2Vec',y_test,lr_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.97      0.69       579\n",
      "           0       0.60      0.12      0.20       333\n",
      "           1       0.72      0.10      0.17       218\n",
      "\n",
      "    accuracy                           0.55      1130\n",
      "   macro avg       0.62      0.40      0.36      1130\n",
      "weighted avg       0.60      0.55      0.45      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf_model = MultinomialNB()\n",
    "nb_tfidf_model.fit(X_train_vectors_tfidf, y_train)\n",
    "nb_tfidf_y_pred = nb_tfidf_model.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,nb_tfidf_y_pred))\n",
    "write_metrics('Naive Bayes','TF-IDF',y_test,nb_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.99      0.68       579\n",
      "           0       0.00      0.00      0.00       333\n",
      "           1       0.27      0.01      0.03       218\n",
      "\n",
      "    accuracy                           0.51      1130\n",
      "   macro avg       0.26      0.34      0.23      1130\n",
      "weighted avg       0.32      0.51      0.35      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "nb_w2v_model = MultinomialNB()\n",
    "nb_w2v_model.fit(X_train_vectors_w2v, y_train)\n",
    "nb_w2v_y_pred = nb_w2v_model.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,nb_w2v_y_pred))\n",
    "write_metrics('Naive Bayes','Word2Vec',y_test,nb_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.81      0.70       579\n",
      "           0       0.49      0.35      0.40       333\n",
      "           1       0.52      0.32      0.39       218\n",
      "\n",
      "    accuracy                           0.58      1130\n",
      "   macro avg       0.54      0.49      0.50      1130\n",
      "weighted avg       0.56      0.58      0.56      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_model = svm.SVC(kernel='linear', random_state=1564)\n",
    "svm_tfidf_model.fit(X_train_vectors_tfidf, y_train)\n",
    "svm_tfidf_y_pred = svm_tfidf_model.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,svm_tfidf_y_pred))\n",
    "write_metrics('SVM','TF-IDF',y_test,svm_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      1.00      0.68       579\n",
      "           0       0.00      0.00      0.00       333\n",
      "           1       0.00      0.00      0.00       218\n",
      "\n",
      "    accuracy                           0.51      1130\n",
      "   macro avg       0.17      0.33      0.23      1130\n",
      "weighted avg       0.26      0.51      0.35      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\utsav\\anaconda3\\envs\\CS418\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "svm_w2v_model = svm.SVC(kernel='linear', random_state=4)\n",
    "svm_w2v_model.fit(X_train_vectors_w2v, y_train)\n",
    "svm_w2v_y_pred = svm_w2v_model.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,svm_w2v_y_pred))\n",
    "write_metrics('SVM','Word2Vec',y_test,svm_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.84      0.67       579\n",
      "           0       0.40      0.21      0.27       333\n",
      "           1       0.42      0.17      0.24       218\n",
      "\n",
      "    accuracy                           0.52      1130\n",
      "   macro avg       0.46      0.40      0.39      1130\n",
      "weighted avg       0.48      0.52      0.47      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "knn_tfidf = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_tfidf_gscv = GridSearchCV(knn_tfidf, param_grid, cv=5)#fit model to data\n",
    "clf = knn_tfidf_gscv.fit(X_train_vectors_tfidf, y_train)\n",
    "knn_tfidf_y_pred = clf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,knn_tfidf_y_pred))\n",
    "write_metrics('KNN','TF-IDF',y_test,knn_tfidf_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.53      0.91      0.67       579\n",
      "           0       0.36      0.12      0.18       333\n",
      "           1       0.29      0.03      0.05       218\n",
      "\n",
      "    accuracy                           0.51      1130\n",
      "   macro avg       0.39      0.35      0.30      1130\n",
      "weighted avg       0.43      0.51      0.41      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "knn_w2v = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_w2v_gscv = GridSearchCV(knn_w2v, param_grid, cv=5)#fit model to data\n",
    "clf_w2v = knn_w2v_gscv.fit(X_train_vectors_w2v, y_train)\n",
    "knn_w2v_y_pred = clf_w2v.predict(X_test_vectors_w2v)\n",
    "print(classification_report(y_test,knn_w2v_y_pred))\n",
    "write_metrics('KNN','Word2Vec',y_test,knn_w2v_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorization</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.5617</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.5605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>0.5116</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>0.4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5504</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.5504</td>\n",
       "      <td>0.4491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.3515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.5551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4843</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.4703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Vectorization  Accuracy  Precision  Recall  F1 Score\n",
       "0  Logistic Regression        TF-IDF    0.5779     0.5617  0.5779    0.5605\n",
       "1  Logistic Regression      Word2Vec    0.5265     0.5116  0.5265    0.4327\n",
       "2          Naive Bayes        TF-IDF    0.5504     0.5952  0.5504    0.4491\n",
       "3          Naive Bayes      Word2Vec    0.5106     0.3154  0.5106    0.3515\n",
       "4                  SVM        TF-IDF    0.5796     0.5612  0.5796    0.5551\n",
       "5                  SVM      Word2Vec    0.5124     0.2625  0.5124    0.3472\n",
       "6                  KNN        TF-IDF    0.5230     0.4843  0.5230    0.4703\n",
       "7                  KNN      Word2Vec    0.5071     0.4312  0.5071    0.4053"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this performance data, we can see that Logistic Regression, Naive Bayes and SVM have performance that is very close. Therefore, I use a Voting Classifier to create a model that predicts based on the votes from all 3 of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.87      0.71       579\n",
      "           0       0.48      0.26      0.34       333\n",
      "           1       0.57      0.25      0.35       218\n",
      "\n",
      "    accuracy                           0.57      1130\n",
      "   macro avg       0.55      0.46      0.47      1130\n",
      "weighted avg       0.56      0.57      0.53      1130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "estimator = [] \n",
    "estimator.append(('LR', LogisticRegression(solver='saga',C=5,penalty='l2',random_state=44))) \n",
    "estimator.append(('Naive Bayes', MultinomialNB())) \n",
    "estimator.append(('SVM', svm.SVC(kernel='linear', random_state=4, probability=True)))\n",
    "voting = VotingClassifier(estimators = estimator, voting ='soft') \n",
    "voting.fit(X_train_vectors_tfidf, y_train) \n",
    "y_pred_vot = voting.predict(X_test_vectors_tfidf) \n",
    "print(classification_report(y_test,y_pred_vot))\n",
    "write_metrics('Voting','TF-IDF',y_test,y_pred_vot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the statistics, we achieve a 57% accuracy using our Voting classifier. But, can we improve things further?<br>\n",
    "If we look back to the start of this notebook, we dropped all records that were not in the classes -1, 0 and 1.<br>\n",
    "However, that means we lose out on 1,544 records that are labelled as class 2 (mixed). These tweets express both positive and negative opinions.<br>\n",
    "In our problem statement, we are only concered with the classes -1, 0 and 1. \n",
    "So, what if we were to perform semi-supervised learning and treat these as unlabelled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>romney claim obama administration public libya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>presidential debate round romney want obama</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>romney lead obama economy job deficit poll fin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>romney obama</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>romney crush obama tonight debate win election...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "2      romney claim obama administration public libya      2\n",
       "9         presidential debate round romney want obama      2\n",
       "15  romney lead obama economy job deficit poll fin...      2\n",
       "19                                       romney obama      2\n",
       "33  romney crush obama tonight debate win election...      2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take the records with class = 2\n",
    "romney_mixed = romney[romney['class'].isin(['2',2])].copy(deep=True)\n",
    "#change class type to int\n",
    "romney_mixed['class']=romney_mixed['class'].astype(int)\n",
    "romney_mixed['tweet_token'] = romney_mixed['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "# remove words with length less than 2\n",
    "romney_mixed['tweet_string'] = romney_mixed['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "# find the frequesncy distribution and remove words with frequency less than 1\n",
    "all_words = ' '.join([text for text in romney_mixed['tweet_string']])\n",
    "tokenized_romney_mixed = nltk.tokenize.word_tokenize(all_words)\n",
    "fdist = FreqDist(tokenized_romney_mixed)\n",
    "romney_mixed['tweet_string_fdist'] = romney_mixed['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "# lemmatize the words\n",
    "romney_mixed['tweet'] = romney_mixed['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "# drop the columns that we do not need\n",
    "romney_mixed = romney_mixed.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "# drop null values\n",
    "romney_mixed.dropna(inplace=True)\n",
    "# drop the class column\n",
    "romney_mixed.drop(['class'], axis=1)\n",
    "print(romney_mixed.shape)\n",
    "romney_mixed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-IDF vectors of these records,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_vectors_tfidf = tfidf_vectorizer.transform(romney_mixed['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the classes for these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_y_pred = lr_model_tfidf.predict(mixed_vectors_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add these classes to our `obama_mixed` dataframe,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>romney claim obama administration public libya</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>presidential debate round romney want obama</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>romney lead obama economy job deficit poll fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>romney obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>romney crush obama tonight debate win election...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "2      romney claim obama administration public libya     -1\n",
       "9         presidential debate round romney want obama     -1\n",
       "15  romney lead obama economy job deficit poll fin...      1\n",
       "19                                       romney obama      0\n",
       "33  romney crush obama tonight debate win election...      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_mixed['class'] = mixed_y_pred\n",
    "romney_mixed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging these records with our existing dataframe `romney_df`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7002, 2)\n"
     ]
    }
   ],
   "source": [
    "romney_merged = romney_df.merge(romney_mixed, how='outer')\n",
    "print(romney_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we retrain our Voting Classifier on this new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df_X = romney_merged['tweet']\n",
    "ss_df_Y = romney_merged['class']\n",
    "ss_X_train, ss_X_test, ss_y_train, ss_y_test = train_test_split(ss_df_X,ss_df_Y,test_size=0.2,random_state = 1551)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweets into TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(ss_X_train)\n",
    "ss_X_test_vectors_tfidf = tfidf_vectorizer.transform(ss_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit them to our Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.86      0.75       703\n",
      "           0       0.61      0.45      0.52       427\n",
      "           1       0.67      0.41      0.51       271\n",
      "\n",
      "    accuracy                           0.65      1401\n",
      "   macro avg       0.65      0.57      0.59      1401\n",
      "weighted avg       0.65      0.65      0.63      1401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utsav\\AppData\\Local\\Temp\\ipykernel_35920\\2285202385.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  performance = performance.append(new_data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "voting.fit(ss_X_train_vectors_tfidf, ss_y_train) \n",
    "y_pred_vot_ss = voting.predict(ss_X_test_vectors_tfidf) \n",
    "print(classification_report(ss_y_test,y_pred_vot_ss))\n",
    "write_metrics('Semi- Supervised Voting','TF-IDF',ss_y_test,y_pred_vot_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This achieves 65% accuracy for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is ready, we can load, clean and tokenize the sample data excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>romney get less minute debate candy crowley still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mitt record character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actually like romney response immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>immigration statement romney answer enough get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man romney dude economics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  romney get less minute debate candy crowley still\n",
       "1                              mitt record character\n",
       "2          actually like romney response immigration\n",
       "3  immigration statement romney answer enough get...\n",
       "4                          man romney dude economics"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading\n",
    "sample_data = pd.ExcelFile(sample_data_excel)\n",
    "sample_romney = pd.read_excel(sample_data, sample_data_sheet)\n",
    "if(len(sample_romney.columns) == 6):\n",
    "    sample_romney = sample_romney[1:]\n",
    "    sample_romney = sample_romney.drop(['Unnamed: 0', 'date', 'time', 'Unnamed: 5'], axis=1)\n",
    "    sample_romney = sample_romney.rename(columns={'Unnamed: 4': 'class', 'Anootated tweet': 'tweet'})\n",
    "    sample_romney = sample_romney[sample_romney['class'].isin(['-1', '0', '1',-1,0,1])]\n",
    "    sample_romney['class']=sample_romney['class'].astype(int)\n",
    "    #create a copy to use for printing to output file\n",
    "    output_data = sample_romney.copy(deep=True)\n",
    "    sample_romney['tweet_token'] = sample_romney['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "    #remove words with length less than 2\n",
    "    sample_romney['tweet_string'] = sample_romney['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "    #Find a frequency distribution, and remove words with frequency less than 1\n",
    "    all_words = ' '.join([text for text in sample_romney['tweet_string']])\n",
    "    tokenized_sample_romney = nltk.tokenize.word_tokenize(all_words)\n",
    "    fdist = FreqDist(tokenized_sample_romney)\n",
    "    sample_romney['tweet_string_fdist'] = sample_romney['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "    sample_romney['tweet'] = sample_romney['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "    sample_romney = sample_romney.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "    sample_romney.dropna(inplace=True)\n",
    "    sample_df_X = sample_romney['tweet']\n",
    "    sample_df_Y = sample_romney['class']\n",
    "elif(len(sample_romney.columns) == 2):\n",
    "    sample_romney = pd.read_excel(sample_data, sample_data_sheet,header=None)\n",
    "    sample_romney.drop(sample_romney.columns[0], axis=1, inplace=True)\n",
    "    sample_romney.columns = ['tweet']\n",
    "    output_data = sample_romney.copy(deep=True)\n",
    "    sample_romney['tweet_token'] = sample_romney['tweet'].apply(lambda stext: tokenize(str(stext)))\n",
    "    #remove words with length less than 2\n",
    "    sample_romney['tweet_string'] = sample_romney['tweet_token'].apply(lambda x:' '.join([item for item in x if len(item)>2]))\n",
    "    #Find a frequency distribution, and remove words with frequency less than 1\n",
    "    all_words = ' '.join([text for text in sample_romney['tweet_string']])\n",
    "    tokenized_sample_romney = nltk.tokenize.word_tokenize(all_words)\n",
    "    fdist = FreqDist(tokenized_sample_romney)\n",
    "    sample_romney['tweet_string_fdist'] = sample_romney['tweet_token'].apply(lambda x: ' '.join([item for item in x if fdist[item] > 1 ]))\n",
    "    sample_romney['tweet'] = sample_romney['tweet_string_fdist'].apply(lambda x: lemmatiser(x))\n",
    "    sample_romney = sample_romney.drop(['tweet_token', 'tweet_string', 'tweet_string_fdist'], axis=1)\n",
    "    sample_romney.dropna(inplace=True)\n",
    "    sample_df_X = sample_romney['tweet']\n",
    "else:\n",
    "    print(\"Invalid columns - check the excel file\")\n",
    "X_sample_vectors = tfidf_vectorizer.transform(sample_df_X)\n",
    "print(sample_romney.shape)\n",
    "sample_romney.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for our sample test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_y_pred = voting.predict(X_sample_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write our predicted classes to an excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_data['Predicted class'] = sample_y_pred\n",
    "# output_data.to_excel(output_file, index=False, sheet_name=sample_data_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(output_file, \"w\")\n",
    "output_file.write(\"(setf x (\\n\")\n",
    "\n",
    "for i in range(len(sample_y_pred)):\n",
    "    line = \"(\"+str(i+1)+\" \"+str(sample_y_pred[i])+\")\"\n",
    "    output_file.write(str(line))\n",
    "    output_file.write('\\n')\n",
    "output_file.write(\"))\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance.to_csv('C:/Users/utsav/OneDrive/UIC/Fall_2023/CS_583/Project/performance_romney.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:<br>\n",
    "https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/ <br>\n",
    "https://medium.com/@qacbustamante/natural-language-processing-using-python-part-1-958d4ea1846e <br>\n",
    "https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/ <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
